{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Copy of knn- blank.ipynb","provenance":[{"file_id":"1mHZRBKUQN5rEF9o2FXEj-XpFQ75-UAO1","timestamp":1602950948935}],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"AU0If7F-iV3s"},"source":["Name: "]},{"cell_type":"code","metadata":{"id":"CZF55UyX86-g","executionInfo":{"status":"ok","timestamp":1602944782934,"user_tz":-480,"elapsed":23999,"user":{"displayName":"Macario Ii Cordel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNL-hn9hauJmIO5qrK9D91Y10HDvQlqtcrNOY1NQ=s64","userId":"14446560774026031618"}},"outputId":"5a052eb3-e5c5-45fb-cc0d-8e065b6a363d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ny1y-7ybiV3u"},"source":["# Programming Assignment 1 : k-Nearest Neighbor (kNN)\n","\n","The kNN classifier consists of two stages:\n","\n","- During training, the classifier takes the training data and simply remembers it\n","- During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples\n","- The value of k is cross-validated\n","\n","In this exercise you will implement these steps and understand the basic classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code."]},{"cell_type":"code","metadata":{"id":"TjMcvl16iV4V","executionInfo":{"status":"ok","timestamp":1602950890029,"user_tz":-480,"elapsed":986,"user":{"displayName":"Macario Ii Cordel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNL-hn9hauJmIO5qrK9D91Y10HDvQlqtcrNOY1NQ=s64","userId":"14446560774026031618"}}},"source":["# Run some setup code for this notebook.\n","import random\n","import numpy as np\n","import pickle\n","import os\n","import matplotlib.pyplot as plt\n","# Makes matplotlib figures appear inline in the notebook\n","# rather than in a new window.\n","%matplotlib inline\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# autoreload external python modules;\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bj-X3v_5iV4c","executionInfo":{"status":"ok","timestamp":1602950641430,"user_tz":-480,"elapsed":970,"user":{"displayName":"Macario Ii Cordel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNL-hn9hauJmIO5qrK9D91Y10HDvQlqtcrNOY1NQ=s64","userId":"14446560774026031618"}}},"source":["def load_CIFAR_batch(filename):\n","    \"\"\" load single batch of cifar \"\"\"\n","    with open(filename, 'rb') as f:\n","        datadict = pickle.load(f, encoding='latin1')\n","        X = datadict['data']\n","        Y = datadict['labels']\n","        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","        Y = np.array(Y)\n","        return X, Y\n","\n","def load_CIFAR10(ROOT):\n","    \"\"\" load all of cifar \"\"\"\n","    xs = []\n","    ys = []\n","    for b in range(1,6):\n","        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n","        X, Y = load_CIFAR_batch(f)\n","        xs.append(X)\n","        ys.append(Y)    \n","    Xtr = np.concatenate(xs)\n","    Ytr = np.concatenate(ys)\n","    del X, Y\n","    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n","    return Xtr, Ytr, Xte, Yte"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfbGUScUiV4h","executionInfo":{"status":"ok","timestamp":1602950901087,"user_tz":-480,"elapsed":6988,"user":{"displayName":"Macario Ii Cordel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNL-hn9hauJmIO5qrK9D91Y10HDvQlqtcrNOY1NQ=s64","userId":"14446560774026031618"}},"outputId":"171e19b0-e131-4a61-c509-acdea977f479","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Load the raw CIFAR-10 data.\n","cifar10_dir = '/content/drive/Shared drives/Machine Learning/Notebooks/Masters (from scratch)/01 - kNN exercise/cifar-10'\n","X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","\n","# Print out the sizes of the training and test data.\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', y_train.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Training data shape:  (50000, 32, 32, 3)\n","Training labels shape:  (50000,)\n","Test data shape:  (10000, 32, 32, 3)\n","Test labels shape:  (10000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XbYdFgFbiV4p"},"source":["# Visualize some examples from the dataset.\n","# We show a few examples of training images from each class.\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","num_classes = len(classes)\n","samples_per_class = 7\n","for y, cls in enumerate(classes):\n","    idxs = np.flatnonzero(y_train == y)\n","    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt_idx = i * num_classes + y + 1\n","        plt.subplot(samples_per_class, num_classes, plt_idx)\n","        plt.imshow(X_train[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_R8Qq4H3iV4u"},"source":["# Subsample the data for more efficient code execution in this exercise\n","num_training = 5000\n","mask = range(num_training)\n","X_train = X_train[mask]\n","y_train = y_train[mask]\n","\n","num_test = 500\n","mask = range(num_test)\n","X_test = X_test[mask]\n","y_test = y_test[mask]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WeKmC_akiV4y"},"source":["# Reshape the image data into rows\n","X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","print(X_train.shape, X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTDwimCgiV44"},"source":["We would now like to classify the test data with our kNN classifier. Recall that we can break down this process into two steps: \n","\n","1. First we must compute the distances between all test examples and all train examples. \n","2. Given these distances, for each test example we find the k nearest examples and have them vote for the label\n","\n","Lets begin with computing the distance matrix between all training and test examples. For example, if there are $N$ training examples and $M$ test examples, this stage should result in a $M \\times N$ matrix where each element $(i,j)$ is the distance between the $i$-th test and $j$-th train example.\n","\n","#### First, open `knn.py` and implement the function `compute_distances_two_loops` that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time."]},{"cell_type":"code","metadata":{"id":"T081ic-NiV45"},"source":["from knn import KNearestNeighbor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tG8A6DWpiV4-"},"source":["classifier = KNearestNeighbor()\n","classifier.train(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwMXK82TiV5E"},"source":["# Test your implementation:\n","dists = classifier.compute_distances_two_loops(X_test)\n","print(dists.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLuZnlfyiV5R"},"source":["#### Now implement the function `predict_labels` and run the code below:"]},{"cell_type":"code","metadata":{"id":"U0a26tEDiV5S"},"source":["# We use k = 1 (which is Nearest Neighbor).\n","y_test_pred = classifier.predict_labels(dists, k=1)\n","\n","# Compute and print the fraction of correctly predicted examples\n","num_correct = np.sum(y_test_pred == y_test)\n","accuracy = float(num_correct) / num_test\n","print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZuuNVFJViV5X"},"source":["You should expect to see approximately `27%` accuracy. Now lets try out a larger `k`, say `k = 5`:"]},{"cell_type":"code","metadata":{"id":"wuQM9ByDiV5X"},"source":["y_test_pred = classifier.predict_labels(dists, k=5)\n","num_correct = np.sum(y_test_pred == y_test)\n","accuracy = float(num_correct) / num_test\n","print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVp7HdF0iV5b"},"source":["You should expect to see a slightly better performance than with `k = 1`."]},{"cell_type":"markdown","metadata":{"id":"OQ1yxl1kiV5c"},"source":["#### Implement the function `compute_distances_one_loop` and run the code below:"]},{"cell_type":"code","metadata":{"id":"L3omnHAtiV5d"},"source":["dists_one = classifier.compute_distances_one_loop(X_test)\n","\n","# To ensure that our vectorized implementation is correct, we make sure that it\n","# agrees with the naive implementation. There are many ways to decide whether\n","# two matrices are similar; one of the simplest is the Frobenius norm. In case\n","# you haven't seen it before, the Frobenius norm of two matrices is the square\n","# root of the squared sum of differences of all elements; in other words, reshape\n","# the matrices into vectors and compute the Euclidean distance between them.\n","difference = np.linalg.norm(dists - dists_one, ord='fro')\n","print('Difference was: %f' % (difference))\n","if difference < 0.001:\n","    print('Good! The distance matrices are the same')\n","else:\n","    print('Uh-oh! The distance matrices are different')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zit90dz3iV5h"},"source":["#### Now implement the fully vectorized version inside `compute_distances_no_loops` and run the code below:"]},{"cell_type":"code","metadata":{"id":"YByRvkq_iV5i"},"source":["dists_two = classifier.compute_distances_no_loops(X_test)\n","\n","# check that the distance matrix agrees with the one we computed before:\n","difference = np.linalg.norm(dists - dists_two, ord='fro')\n","print('Difference was: %f' % (difference))\n","if difference < 0.001:\n","    print('Good! The distance matrices are the same')\n","else:\n","    print('Uh-oh! The distance matrices are different')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vyeO8x2uiV5n"},"source":["### Let's compare how fast the implementations are"]},{"cell_type":"code","metadata":{"id":"fBtTL11yiV5o"},"source":["def time_function(f, *args):\n","    \"\"\"\n","    Call a function f with args and return the time (in seconds) that it took to execute.\n","    \"\"\"\n","    import time\n","    tic = time.time()\n","    f(*args)\n","    toc = time.time()\n","    return toc - tic\n","\n","two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)\n","print('Two loop version took %f seconds' % two_loop_time)\n","\n","one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)\n","print('One loop version took %f seconds' % one_loop_time)\n","\n","no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)\n","print('No loop version took %f seconds' % no_loop_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRUZ2Q59iV5r"},"source":["You should see significantly faster performance with the fully vectorized implementation"]},{"cell_type":"markdown","metadata":{"id":"_zxNNXI9iV5s"},"source":["### Cross-validation\n","\n","We have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation."]},{"cell_type":"code","metadata":{"id":"Ac85XwfUiV5t"},"source":["num_folds = 5\n","k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n","\n","X_train_folds = []\n","y_train_folds = []\n","################################################################################\n","# TODO:                                                                        #\n","# Split up the training data into folds. After splitting, X_train_folds and    #\n","# y_train_folds should each be lists of length num_folds, where                #\n","# y_train_folds[i] is the label vector for the points in X_train_folds[i].     #\n","# Hint: Look up the numpy array_split function.                                #\n","################################################################################\n","pass\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################\n","\n","# A dictionary holding the accuracies for different values of k that we find\n","# when running cross-validation. After running cross-validation,\n","# k_to_accuracies[k] should be a list of length num_folds giving the different\n","# accuracy values that we found when using that value of k.\n","k_to_accuracies = {}\n","\n","\n","################################################################################\n","# TODO:                                                                        #\n","# Perform k-fold cross validation to find the best value of k. For each        #\n","# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #\n","# where in each case you use all but one of the folds as training data and the #\n","# last fold as a validation set. Store the accuracies for all fold and all     #\n","# values of k in the k_to_accuracies dictionary.                               #\n","################################################################################\n","\n","pass\n","\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################\n","\n","# Print out the computed accuracies\n","for k in sorted(k_to_accuracies):\n","    for accuracy in k_to_accuracies[k]:\n","        print('k = %d, accuracy = %f' % (k, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZOhVHsIiV5w"},"source":["# plot the raw observations\n","for k in k_choices:\n","    accuracies = k_to_accuracies[k]\n","    plt.scatter([k] * len(accuracies), accuracies)\n","\n","# plot the trend line with error bars that correspond to standard deviation\n","accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n","accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n","plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n","plt.title('Cross-validation on k')\n","plt.xlabel('k')\n","plt.ylabel('Cross-validation accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7OPDSoziV52"},"source":["# Based on the cross-validation results above, choose the best value for k,   \n","# retrain the classifier using all the training data, and test it on the test\n","# data. You should be able to get above 28% accuracy on the test data.\n","best_k = 1\n","\n","classifier = KNearestNeighbor()\n","classifier.train(X_train, y_train)\n","y_test_pred = classifier.predict(X_test, k=best_k)\n","\n","# Compute and display the accuracy\n","num_correct = np.sum(y_test_pred == y_test)\n","accuracy = float(num_correct) / num_test\n","print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G44h0VIiiV56"},"source":["Obviously, kNN is not a good algorithm for images or high dimensional data in general. However, in many problems it can still be a tough to beat baseline."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"UkYPPVWfiV57"},"source":["## Acknowledgements\n","This exercise is modified version of the exercise from cs231n class at Stanford University."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"b1df6GtriV58"},"source":["\n","\n","<!-- DO NOT MODIFY OR DELETE THIS -->\n","<sup>made/compiled by daniel stanley tan & courtney anne ngo üê∞ & thomas james tiam-lee</sup> <br>\n","<sup>for comments, corrections, suggestions, please email:</sup><sup> danieltan07@gmail.com & courtneyngo@gmail.com & thomasjamestiamlee@gmail.com</sup><br>\n","<sup>please cc your instructor, too</sup>\n","<!-- DO NOT MODIFY OR DELETE THIS -->\n","\n","\n"]}]}